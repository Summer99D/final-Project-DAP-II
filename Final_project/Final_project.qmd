---

title: "DAP II:Final Project"
author: "Genevieve Madigan, Summer Negahdar, Jenny Zhong"
date: "Fall 2024"
format: html
execute:
  echo: false
  eval: true


---
## the link to google drive for data

https://drive.google.com/drive/folders/1U2egyK2B7HBMOuB5wlj-HAs8HkjFLrJk?usp=drive_link

# workflow and team members
Genevieve Madigan: #write your github Id here: 
-responsibility: write up and data visualization
Summer Negahdar: Summer99D
-responsibility: creation of shiny app and data visualization
Jenny Zhong: datapolicypython
-responsibility: data cleaning and preparation


## Introduction and prior articles




## Data Cleaning

 
the workflow of the things we needed to do:
1. upload temp app,
2. save the file as df and as csv file (in the data folder)
3. upload crime rate
4. clean crime df
5. save it as df and as csv (in data folder)
6. merge both dfs based on ZIPCODES
7. make a scatterplot of crime rate and temperature
8. make a base map (shapely)
9. make a heated map using scatterplot and base map
10. save the final merged csv(in data folder)
11. shiny app
12. (extra credit): use NLP for finding crime-heat (it sohuld not be hard I'd say)



```{python}
##installing the necessary packages
import pandas as pd
import altair as alt
import numpy as np
import geopandas as gpd
import shapely
import shiny
from shapely.geometry import Point
import pgeocode
```


# Load your CSV file
# Crime data: Merging crime data together and merging crime data with ZIP Code
1. Loading data together

After the presentation, we decided that we would shorten our datasets to 2010 - 2015. Subsequently, the code we have accommodates that below. 
```{python}
crimes10 = pd.read_csv('Crimes2010.csv')
crimes1112 = pd.read_csv('Crimes20112012.csv')
crimes131415 = pd.read_csv('Crimes201320142015.csv')
```

2. Examining the columns of crime data 
```{python}
print("2010 Columns:", crimes10.columns)
```

3. Merging all datasets together
```{python}
totalcrimedata = pd.concat([crimes2010, crimes1112, crimes131415])
``` 

Examining the merged dataset
```{python}
print(totalcrimedata.info())
```


Whether all years exist
```{python}
print(totalcrimedata['Year'].value_counts())
```

Summary statistics for numerical columns 
```{python}
print(totalcrimedata.describe())
```

Converting latitude and longitude to ZIP Codes
```{python}
len(totalcrimedata)
```

Load ZIP code shapefiles
```{python}
zip_shapes = gpd.read_file("/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/ZIP Code Shapefiles/tl_2015_us_zcta510.shp")
```

```{python}
zip_shapes = zip_shapes.to_crs("EPSG:4326")
print(zip_shapes.head())
```

crime geometry
```{python}
geometry = [Point(xy) for xy in zip(totalcrimedata['Longitude'], totalcrimedata['Latitude'])]
```

```{python}
crime_gdf = gpd.GeoDataFrame(totalcrimedata, geometry=geometry, crs="EPSG:4326") 
print(crime_gdf.head())
```

```{python}
crime_gdf['geometry'] = crime_gdf.geometry.buffer(0.01)
```

Merging two data sets
```{python}
matched_data = gpd.sjoin(crime_gdf, zip_shapes, how="left", predicate="intersects")

# matched_data.to_csv("filtered_crime_data.csv", index=False)
```

Clean data
```{python}
matched_data.drop(columns=['ID', 'District', 'Community Area', 'CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10', 'FBI Code', 'Updated On', 'GEOID10'], inplace=True)
```

Operate from CSV file
```{python}
matched_data = pd.read_csv("filtered_crime_data.csv")
print(matched_data.head())
```

Drop appropriate columns 
```{python}
matched_data.drop(columns=['ID', 'District', 'Community Area', 'CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10', 'FBI Code', 'Updated On', 'GEOID10'], inplace=True)
print(matched_data.head())
```

```{python}
matched_data['Date'] = pd.to_datetime(matched_data['Date'], errors='coerce')
```

```{python}
matched_data['YearMonthDay'] = matched_data['Date'].dt.date
matched_data['Time'] = matched_data['Date'].dt.time
```

```{python}
matched_data.drop(columns=['Date', 'Time', 'GEOID10', 'CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10', 'Updated On', 'FBI Code'], inplace=True)
matched_data.head(5)
```

```{python}
columns = matched_data.columns.tolist()
columns.remove('YearMonthDay')
columns.insert(columns.index('Case Number') + 1, 'YearMonthDay')
matched_data = matched_data[columns]
print(matched_data.columns)
```

```{python}
matched_data.to_csv("filtered_crime_data.csv", index=False)
```

# Merging weather data together and merging weather data with ZIP code 

Merging all datasets together
```{python}
weather2010 = pd.read_csv('AT2010.csv')
weather2011 = pd.read_csv('AT2011.csv')
weather2012 = pd.read_csv('AT2012.csv')
weather2013 = pd.read_csv('AT2013.csv')
weather2014 = pd.read_csv('AT2014.csv')
weather2015 = pd.read_csv('AT2015.csv')
```

```{python}
total_weather = pd.concat([weather2010, weather2011, weather2012, weather2013, weather2014, weather2015], ignore_index=True)
```

Convert weather data to geodataframe
```{python}
geometry = [Point(xy) for xy in zip(total_weather['LONGITUDE'], total_weather['LATITUDE'])]

weather_gdf = gpd.GeoDataFrame(total_weather, geometry=geometry)

weather_gdf.set_crs("EPSG:4326", inplace=True)

weather_gdf['geometry'] = weather_gdf.geometry.buffer(0.01)
```

Load the ZIP Code Shapefile
```{python}
zip_shapefile = gpd.read_file('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/ZIP_Code_Shapefiles/tl_2015_us_zcta510.shp')

zip_shapefile = zip_shapefile.to_crs("EPSG:4326")
```

Spatial join to match the zip codes
```{python}
weather_with_zip = gpd.sjoin(weather_gdf, zip_shapefile, how='left', predicate='intersects')

print(weather_with_zip.head())
```

Drop irrelevant dataframes: 
```{python}
weather_with_zip.drop(["CLASSFP10", "MTFCC10", "FUNCSTAT10", "ALAND10", "AWATER10", "GEOID10"], axis=1, inplace=True)

print(weather_with_zip)
```

Final CSV file for weather 
```{python}
weather_with_zip.to_csv("weather_with_zip.csv", index=False)
```

Loading CSV file 
```{python}
weatherfinal = pd.read_csv('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/Final project/Weather_Data/weather_with_zip.csv')
```

```{python}
weatherfinal.head(10)
weatherfinal = weatherfinal.drop(columns=['TMAX', 'TMIN', 'ELEVATION', 'index_right'])
```

```{python}
weatherfinal.to_csv('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/Final project/Weather_Data/weatherfinal_cleaned.csv', index=False)
```

