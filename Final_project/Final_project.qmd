---

title: "DAP II:Final Project"
author: "Genevieve Madigan, Summer Negahdar, Jenny Zhong"
date: "Fall 2024"
format: html

---
# workflow and team members
Genevieve Madigan: #write your github Id here: Madigan989
-responsibility: write up and data visualization
Summer Negahdar: Summer99D
-responsibility: creation of shiny app and data visualization
Jenny Zhong: datapolicypython
-responsibility: data cleaning and preparation


## Introduction and prior articles




## Data Cleaning

To Jenny and Gena: 
things we need to do:
1. upload temp app,


# Load your CSV file
# Crime data: Merging crime data together and merging crime data with ZIP Code
1. Loading data together
```{python}
import pandas as pd
import geopandas as gpd
import pandas as pd
from shapely.geometry import Point
```

After the presentation, we decided that we would shorten our datasets to 2010 - 2015 so it would be easier to load into our laptops. Subsequently, the code we have accommodates this change below. 

First, we load the CSV datasets into dataframes into python
```{python}
crimes10 = pd.read_csv('Crimes2010.csv')
crimes1112 = pd.read_csv('Crimes20112012.csv')
crimes131415 = pd.read_csv('Crimes201320142015.csv')
```

Then we examine the columns of crime data 
```{python}
print("2010 Columns:", crimes10.columns)
```

We then merge the datasets together so we have a crime rate dataset from 2010 - 2015 
```{python}
totalcrimedata = pd.concat([crimes2010, crimes1112, crimes131415])
``` 

We then want to examine the merged dataset
```{python}
print(totalcrimedata.info())
```

This ensures that all years exist
```{python}
print(totalcrimedata['Year'].value_counts())
```

Summary statistics for numerical columns 
```{python}
print(totalcrimedata.describe())
```

# Now we will convert the longitude and latitude in the crime data into ZIP Code by using an external shapefile and cross referencing this shapefule with our crime dataset
Load ZIP code shapefiles
```{python}
zip_shapes = gpd.read_file("/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/ZIP Code Shapefiles/tl_2015_us_zcta510.shp")
```

Here, we are transforming spatial data into a consistent coordinate reference system (EPSG:4326), creating geometric points from longitude and latitude, and combining this data into a GeoDataFrame for spatial analysis.
```{python}
zip_shapes = zip_shapes.to_crs("EPSG:4326")
print(zip_shapes.head())
```

```{python}
geometry = [Point(xy) for xy in zip(totalcrimedata['Longitude'], totalcrimedata['Latitude'])]
```

```{python}
crime_gdf = gpd.GeoDataFrame(totalcrimedata, geometry=geometry, crs="EPSG:4326") 
print(crime_gdf.head())
```

Then we are creating buffer zones around each geometry point in crime_gdf with a radius of 0.01 units, modifying the geometry column to reflect these expanded areas.
```{python}
crime_gdf['geometry'] = crime_gdf.geometry.buffer(0.01)
```

We are then performing a spatial join to match crime_gdf (containing crime data) with zip_shapes based on their spatial intersection, and saving the resulting data to a CSV file.
```{python}
matched_data = gpd.sjoin(crime_gdf, zip_shapes, how="left", predicate="intersects")

matched_data.to_csv("filtered_crime_data.csv", index=False)
```

Operate from CSV file
```{python}
matched_data = pd.read_csv("filtered_crime_data.csv")
print(matched_data.head())
```

Drop appropriate columns 
```{python}
matched_data.drop(columns=['ID', 'District', 'Community Area', 'CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10', 'FBI Code', 'Updated On', 'GEOID10'], inplace=True)
print(matched_data.head())
```

Convert the Date column to a datetime format
```{python}
matched_data['Date'] = pd.to_datetime(matched_data['Date'], errors='coerce')
```

Create new columns (YearMonthDay and Time) to separately store the date and time components of the Date column. 
```{python}
matched_data['YearMonthDay'] = matched_data['Date'].dt.date
matched_data['Time'] = matched_data['Date'].dt.time
```

Rearrange the columns to place YearMonthDay immediately after Case Number for better organization
```{python}
columns = matched_data.columns.tolist()
columns.remove('YearMonthDay')
columns.insert(columns.index('Case Number') + 1, 'YearMonthDay')
matched_data = matched_data[columns]
print(matched_data.columns)
```

Save the cleaned and restructured data to a CSV file
```{python}
matched_data.to_csv("filtered_crime_data.csv", index=False)
```

# Merging weather data together and merging weather data with ZIP code 

Merging all 2010 - 2015 datasets together. The process is similar to the above. 
```{python}
weather2010 = pd.read_csv('AT2010.csv')
weather2011 = pd.read_csv('AT2011.csv')
weather2012 = pd.read_csv('AT2012.csv')
weather2013 = pd.read_csv('AT2013.csv')
weather2014 = pd.read_csv('AT2014.csv')
weather2015 = pd.read_csv('AT2015.csv')
```

```{python}
total_weather = pd.concat([weather2010, weather2011, weather2012, weather2013, weather2014, weather2015], ignore_index=True)
```

Convert weather data to geodataframe
```{python}
geometry = [Point(xy) for xy in zip(total_weather['LONGITUDE'], total_weather['LATITUDE'])]

weather_gdf = gpd.GeoDataFrame(total_weather, geometry=geometry)

weather_gdf.set_crs("EPSG:4326", inplace=True)

weather_gdf['geometry'] = weather_gdf.geometry.buffer(0.01)
```

Load the ZIP Code Shapefile
```{python}
zip_shapefile = gpd.read_file('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/ZIP_Code_Shapefiles/tl_2015_us_zcta510.shp')

zip_shapefile = zip_shapefile.to_crs("EPSG:4326")
```

Spatial join to match the zip codes
```{python}
weather_with_zip = gpd.sjoin(weather_gdf, zip_shapefile, how='left', predicate='intersects')

print(weather_with_zip.head())
```

Drop irrelevant dataframes: 
```{python}
weather_with_zip.drop(["CLASSFP10", "MTFCC10", "FUNCSTAT10", "ALAND10", "AWATER10", "GEOID10"], axis=1, inplace=True)

print(weather_with_zip)
```

Final CSV file for weather 
```{python}
weather_with_zip.to_csv("weather_with_zip.csv", index=False)
```

Loading CSV file 
```{python}
weatherfinal = pd.read_csv('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/Final project/Weather_Data/weather_with_zip.csv')
```

Dropping unnecessary columns
```{python}
weatherfinal.head(10)
weatherfinal = weatherfinal.drop(columns=['TMAX', 'TMIN', 'ELEVATION', 'index_right'])
```

Saving to a CSV file 
```{python}
weatherfinal.to_csv('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/Final project/Weather_Data/weatherfinal_cleaned.csv', index=False)
```

# Apply function to your data


# Save the updated dataframe to a new CSV




2. save the file as df and as csv file (in the data folder)
3. upload crime rate
4. clean crime df
5. save it as df and as csv (in data folder)
6. merge both dfs based on ????
7. make a scatterplot of crime rate and temperature
8. make a base map (shapely)
9. make a heated map using scatterplot and base map
10. save the final merged csv(in data folder)
11. shiny app
12. (extra credit): use NLP for finding crime-heat (it sohuld not be hard I'd say)



```{python}
##installing the necessary packages
import pandas as pd
import matplotlib.pyplot as plt
import altair as alt
import numpy as np
import geopandas as gpd
import shapely
import shiny
#from shapely.geometry import Point
#import pgeocode
```



## Data experiment


## Data Visualization
#merging the data
#For the following data, we decided to filter the information down to 
#2010-2015 because 30 GB is way too much, the following code is how we 
#shrunk the data 

```{python}
#final_crime_data = pd.read_csv(r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\final_crime_data.csv')
#final_weather_data = pd.read_csv(r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\final_weather_data.csv')
```

```{python}
#final_crime_data = pd.read_csv(r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\final_crime_data.csv')
#final_crime_data['DATE'] = pd.to_datetime(final_crime_data['YearMonthDay'])
#filtered_crime_data = final_crime_data[
#    (final_crime_data['DATE'].dt.year >= 2010) & (final_crime_data['DATE'].dt.year <= 2015)
#]
#filtered_crime_data.to_csv('filtered_crime_data.csv', index=False)
```

So this is the filtered data from 2010-2015

Crime data grouping and removing unneccesary columns 

And making a function to group crime types

```{python}
filtered_weather_data = pd.read_csv(r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\Final project\weatherfinal_cleaned.csv')
filtered_crime_data = pd.read_csv(r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\Final project\filtered_crime_data.csv')
```


```{python}

unique_primary_types = filtered_crime_data['Primary Type'].unique()

def categorize_crime(crime_type):
    if crime_type in ['BATTERY', 'ASSAULT', 'ROBBERY', 'SEX OFFENSE', 
                      'CRIM SEXUAL ASSAULT', 'CRIMINAL SEXUAL ASSAULT', 
                      'HOMICIDE', 'KIDNAPPING', 'STALKING', 'INTIMIDATION']:
        return 'Violent Crimes'
    elif crime_type in ['MOTOR VEHICLE THEFT', 'THEFT', 'BURGLARY', 
                        'CRIMINAL DAMAGE', 'ARSON', 'CRIMINAL TRESPASS']:
        return 'Property Crimes'
    elif crime_type in ['WEAPONS VIOLATION', 'PUBLIC PEACE VIOLATION', 
                        'LIQUOR LAW VIOLATION', 'OBSCENITY', 
                        'PUBLIC INDECENCY', 'CONCEALED CARRY LICENSE VIOLATION']:
        return 'Public Order and Regulatory Violations'
    else:
        return 'Other Offenses'

filtered_crime_data['Crime Type Group'] = filtered_crime_data['Primary Type'].apply(categorize_crime)

columns_to_drop = [
    'YearMonthDay', 'Block', 'IUCR', 'Primary Type', 'Description',
    'Location Description', 'Arrest', 'Domestic', 'Beat', 'Ward',
    'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Location',
    'ID', 'District', 'Community Area', 'index_right', 'INTPTLAT10', 
    'INTPTLON10'
]

filtered_crime_data = filtered_crime_data.drop(columns=columns_to_drop)

filtered_crime_data = filtered_crime_data.dropna(subset=['ZCTA5CE10'])
filtered_crime_data['ZCTA5CE10'] = filtered_crime_data['ZCTA5CE10'].astype(int)

filtered_crime_data.rename(columns={'ZCTA5CE10': 'ZIP_CODE'}, inplace=True)

print(filtered_crime_data.head())
```

Merging

```{python}

# Ensure DATE columns are in datetime format
filtered_crime_data['DATE'] = pd.to_datetime(filtered_crime_data['DATE'])
filtered_weather_data['DATE'] = pd.to_datetime(filtered_weather_data['DATE'])

# Group the crime data to create a Crime Count column
crime_summary = filtered_crime_data.groupby(['DATE', 'ZIP_CODE', 'Crime Type Group']).size().reset_index(name='Crime Count')

# Merge the grouped crime summary with the weather data on DATE and ZIP_CODE
merged_data = pd.merge(
    crime_summary,
    filtered_weather_data[['DATE', 'ZIP_CODE', 'TAVG', 'geometry']],  # Select relevant weather columns
    on=['DATE', 'ZIP_CODE'],
    how='inner'  # Use 'inner' to match rows in both datasets
)

# Display the first few rows of the merged dataset
print(merged_data.head())

# Save the merged dataset to a CSV file
merged_data.to_csv(r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\Final project\merged_data.csv', index=False)


```



#Now that the data is filtered lets make the two charts


#BarChart

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Bin the temperature data into 15-unit ranges
filtered_weather_data['TAVG_bin'] = pd.cut(
    filtered_weather_data['TAVG'], 
    bins=range(int(filtered_weather_data['TAVG'].min()) // 15 * 15, 
               int(filtered_weather_data['TAVG'].max()) // 15 * 15 + 15, 
               15), 
    right=False
)

# Merge crime and weather data to keep crime group info
merged_data_with_bins = pd.merge(
    merged_data,
    filtered_weather_data[['DATE', 'TAVG_bin']], 
    on='DATE',
    how='left'
)

# Group data by TAVG_bin and Crime Type Group
crime_distribution = merged_data_with_bins.groupby(['TAVG_bin', 'Crime Type Group'])['Crime Count'].sum().unstack(fill_value=0)

# Plot stacked bar chart
crime_distribution.plot(kind='bar', stacked=True, figsize=(12, 7), width=0.9)
plt.title('Crime Distribution by Temperature Range (Binned by 15)')
plt.xlabel('Temperature Range (°F)')
plt.ylabel('Total Crime Count')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.legend(title='Crime Type Group')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


```


#Aggregated time series
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Load the merged dataset
merged_data_path = r'C:\Users\madig\Documents\Github\Year 2024-2025\final-Project-DAP-II\Final project\merged_data.csv'
merged_data = pd.read_csv(merged_data_path)

# Ensure DATE column is in datetime format
merged_data['DATE'] = pd.to_datetime(merged_data['DATE'])

# Aggregate data by Date across all ZIP codes
all_zip_time_series = merged_data.groupby('DATE').agg(
    total_crime=('Crime Count', 'sum'),  # Sum up crime counts
    avg_temp=('TAVG', 'mean')  # Average temperature
).reset_index()

# Plot the time-series
plt.figure(figsize=(12, 6))

# Plot Crime Count
plt.plot(all_zip_time_series['DATE'], all_zip_time_series['total_crime'], label='Total Crime', color='red')

# Plot Average Temperature
plt.plot(all_zip_time_series['DATE'], all_zip_time_series['avg_temp'], label='Average Temperature (TAVG)', color='blue')

# Add titles and labels
plt.title('City-Wide Crime and Temperature Trends Over Time', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Count / Temperature', fontsize=12)
plt.legend()
plt.grid(True)

# Display the plot
plt.show()

```



## Shiny App





