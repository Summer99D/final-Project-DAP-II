{
  "hash": "bbc5f25b303cd5756507ed5328477018",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"DAP II: Final Project\"\n\nauthor: \"Genevieve Madigan, Summer Negahdar, Jenny Zhong\"\n\ndate: \"Fall 2024\"\n\nformat: \n  html:\n    default-code-block-options:\n      echo: false\nexecute:\n  freeze: true\n---\n\n\n# Workflow and team members\n\nGenevieve Madigan: Madigan989\n-responsibility: write up and data visualization\nSummer Negahdar: Summer99D\n-responsibility: creation of shiny app and data visualization\nJenny Zhong: datapolicypython\n-responsibility: data cleaning and preparation\n\n\n# Introduction and prior articles\n\n## Research Question\nOur project investigates the impact of temperature on crime rates in Chicago, focusing on both violent and non-violent crimes. Specifically, our research question is: How does weather affect the occurrence and types of crimes within specific ZIP codes in Chicago?\nInspired by a study published in the Journal of Criminal Justice (2015–2021) analyzing the temperature-crime relationship across multiple cities, we extended this work to a different time frame (2010–2015). This allowed us to test the study's conclusions during a period of varying political leadership, economic conditions, and macro trends. Understanding these dynamics can inform public safety strategies and resource allocation in response to climate patterns.\nPrior research, including the temperature-aggression hypothesis (Kahn, Deschenes, and Greenstone, 2009) and routine activity theory (Gottfredson and Hirschi, 1990), shows that environmental factors significantly affect criminal activity. These frameworks suggest higher temperatures may increase aggression, while weather influences daily activities, affecting crime opportunities. \n\n## Methodology\nTo conduct our analysis, we utilized two primary datasets. The crime data was retrieved from the Chicago Data Portal, spanning the years 2004 to 2015. Due to the large size of this dataset, we narrowed our scope to focus on the period from 2010 to 2015 to make the analysis more computationally manageable. Weather data was acquired from the National Oceanic and Atmospheric Administration’s (NOAA) daily summaries, providing average, minimum, and maximum temperatures for the city of Chicago.\nTo analyze the spatial relationship between crime and weather data, we mapped latitude and longitude coordinates to ZIP Codes using ZIP Code Tabulation Area (ZCTA) shapefiles from the U.S. Census Bureau’s TIGER/Line database. The data preparation process involved several steps. First, we loaded the ZCTA shapefiles into Python as GeoDataFrames using the geopandas library and reprojected them to the EPSG:4326 coordinate system to ensure compatibility with the datasets.\nNext, latitude and longitude values from the crime and weather datasets were converted into shapely.geometry.Point objects to create geospatial geometries. To address precision issues near ZCTA boundaries, small buffers were applied to these points, which slightly expanded their spatial coverage. This step improved the accuracy of subsequent spatial joins by reducing mismatches caused by minor spatial discrepancies.\nFinally, a spatial join was performed using the gpd.sjoin() function to map each point to its corresponding ZCTA polygon. These ZCTAs were then translated into ZIP Codes, allowing us to align the crime and weather data spatially for analysis. This process ensured that each data point was correctly associated with its geographical context, enabling a robust examination of the relationship between temperature and crime across Chicago.\n\n## Challenges\nThe crime dataset initially posed significant challenges due to its size, totaling approximately 30GB. This required substantial computational resources for processing and analysis. To address this, we reduced the analysis time frame to 2010–2015, which decreased the dataset size to 13GB. While this adjustment made the dataset more manageable, it still presented difficulties in terms of sharing and processing efficiently. Additionally, to visualize the data and build a functional Shiny app, we had to sample only 1% of the dataset to ensure the app could handle the data volume without performance issues.\nGeospatial analysis introduced further complexities. Mapping latitude and longitude coordinates to ZIP Codes required intensive computations, particularly when working with large datasets. Loading geographic centroids and resolving mismatches during spatial joins added delays to the workflow, highlighting the challenges of working with spatial data at this scale.\nAnother issue we encountered was standardizing date formats across the crime and weather datasets. These inconsistencies necessitated additional preprocessing to align timeframes accurately, ensuring that the data could be integrated seamlessly for analysis. This step was critical to maintaining the temporal consistency required for our study.\n\n## Methodology with codes \n# Crime data: Merging crime data together and merging crime data with ZIP Code\n1. Loading data together\n\n::: {#02aaf4e1 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport geopandas as gpd\nimport shapely\nimport pandas as pd\nfrom shapely.geometry import Point\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport numpy as np\nimport shiny\n```\n:::\n\n\nAs mentioned above, after the presentation, we decided that we would shorten our datasets to 2010 - 2015 so it would be easier to load into our laptops. Subsequently, the code we have accommodates this change below. \n\nFirst, we load the CSV datasets into dataframes into python\n\n::: {#d78324c9 .cell execution_count=2}\n``` {.python .cell-code}\n#crimes10 = pd.read_csv('Crimes2010.csv')\n#crimes1112 = pd.read_csv('Crimes20112012.csv')\n#crimes131415 = pd.read_csv('Crimes201320142015.csv')\n```\n:::\n\n\nThen we examine the columns of crime data \n\n::: {#a0354f0c .cell execution_count=3}\n``` {.python .cell-code}\n#print(\"2010 Columns:\", crimes10.columns)\n```\n:::\n\n\nWe then merge the datasets together so we have a crime rate dataset from 2010 - 2015 and examine it.\n\n::: {#c49a51bb .cell execution_count=4}\n``` {.python .cell-code}\n#totalcrimedata = pd.concat([crimes2010, crimes1112, crimes131415])\n#print(totalcrimedata.info())\n```\n:::\n\n\nSummary statistics for numerical columns \n\n::: {#a06507a2 .cell execution_count=5}\n``` {.python .cell-code}\n#print(totalcrimedata.describe())\n```\n:::\n\n\n# Now we will convert the longitude and latitude in the crime data into ZIP Code by using an external shapefile and cross referencing this shapefile with our crime dataset\nLoad ZIP code shapefiles\n\n::: {#033e65b2 .cell execution_count=6}\n``` {.python .cell-code}\n#zip_shapes = gpd.read_file(\"/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/ZIP Code Shapefiles/tl_2015_us_zcta510.shp\")\n```\n:::\n\n\nHere, we are transforming spatial data into a consistent coordinate reference system (EPSG:4326), creating geometric points from longitude and latitude, and combining this data into a GeoDataFrame for spatial analysis.\n\n::: {#2da6d31e .cell execution_count=7}\n``` {.python .cell-code}\n#zip_shapes = zip_shapes.to_crs(\"EPSG:4326\")\n#print(zip_shapes.head())\n```\n:::\n\n\n::: {#c4b5dcc1 .cell execution_count=8}\n``` {.python .cell-code}\n#geometry = [Point(xy) for xy in zip(totalcrimedata['Longitude'], totalcrimedata['Latitude'])]\n```\n:::\n\n\n::: {#5b2202b1 .cell execution_count=9}\n``` {.python .cell-code}\n#crime_gdf = gpd.GeoDataFrame(totalcrimedata, geometry=geometry, crs=\"EPSG:4326\") \n#print(crime_gdf.head())\n```\n:::\n\n\nThen we are creating buffer zones around each geometry point in crime_gdf with a radius of 0.01 units, modifying the geometry column to reflect these expanded areas.\n\n::: {#dd3f8349 .cell execution_count=10}\n``` {.python .cell-code}\n#crime_gdf['geometry'] = crime_gdf.geometry.buffer(0.01)\n```\n:::\n\n\nWe are then performing a spatial join to match crime_gdf (containing crime data) with zip_shapes based on their spatial intersection, and saving the resulting data to a CSV file.\n\n::: {#7ea72c69 .cell execution_count=11}\n``` {.python .cell-code}\n#matched_data = gpd.sjoin(crime_gdf, zip_shapes, how=\"left\", predicate=\"intersects\")\n\n#matched_data.to_csv(\"filtered_crime_data.csv\", index=False)\n```\n:::\n\n\nOperate from CSV file\n\n::: {#a6d18d36 .cell execution_count=12}\n``` {.python .cell-code}\n#matched_data = pd.read_csv(\"filtered_crime_data.csv\")\n#print(matched_data.head())\n```\n:::\n\n\nDrop appropriate columns \n\n::: {#f6ede5c8 .cell execution_count=13}\n``` {.python .cell-code}\n#matched_data.drop(columns=['ID', 'District', 'Community Area', 'CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10', 'FBI Code', 'Updated On', 'GEOID10'], inplace=True)\n#print(matched_data.head())\n```\n:::\n\n\nConvert the Date column to a datetime format\n\n::: {#ea0e7e87 .cell execution_count=14}\n``` {.python .cell-code}\n#matched_data['Date'] = pd.to_datetime(matched_data['Date'], errors='coerce')\n```\n:::\n\n\nCreate new columns (YearMonthDay and Time) to separately store the date and time components of the Date column. \n\n::: {#b5fbfaa7 .cell execution_count=15}\n``` {.python .cell-code}\n#matched_data['YearMonthDay'] = matched_data['Date'].dt.date\n#matched_data['Time'] = matched_data['Date'].dt.time\n```\n:::\n\n\nRearrange the columns to place YearMonthDay immediately after Case Number for better organization\n\n::: {#1d3978ce .cell execution_count=16}\n``` {.python .cell-code}\n#columns = matched_data.columns.tolist()\n#columns.remove('YearMonthDay')\n#columns.insert(columns.index('Case Number') + 1, 'YearMonthDay')\n#matched_data = matched_data[columns]\n#print(matched_data.columns)\n```\n:::\n\n\nSave the cleaned and restructured data to a CSV file\n\n::: {#df8776a9 .cell execution_count=17}\n``` {.python .cell-code}\n#matched_data.to_csv(\"filtered_crime_data.csv\", index=False)\n```\n:::\n\n\n## Merging weather data together and merging weather data with ZIP code \n\nMerging all 2010 - 2015 datasets together. The process is similar to the above. \n\n::: {#8ff69b19 .cell execution_count=18}\n``` {.python .cell-code}\n#weather2010 = pd.read_csv('AT2010.csv')\n#weather2011 = pd.read_csv('AT2011.csv')\n#weather2012 = pd.read_csv('AT2012.csv')\n#weather2013 = pd.read_csv('AT2013.csv')\n#weather2014 = pd.read_csv('AT2014.csv')\n#weather2015 = pd.read_csv('AT2015.csv')\n```\n:::\n\n\n::: {#79e90acd .cell execution_count=19}\n``` {.python .cell-code}\n#total_weather = pd.concat([weather2010, weather2011, weather2012, weather2013, weather2014, weather2015], ignore_index=True)\n```\n:::\n\n\nConvert weather data to geodataframe\n\n::: {#bc213aa2 .cell execution_count=20}\n``` {.python .cell-code}\n#geometry = [Point(xy) for xy in zip(total_weather['LONGITUDE'], total_weather['LATITUDE'])]\n\n#weather_gdf = gpd.GeoDataFrame(total_weather, geometry=geometry)\n\n#weather_gdf.set_crs(\"EPSG:4326\", inplace=True)\n\n#weather_gdf['geometry'] = weather_gdf.geometry.buffer(0.01)\n```\n:::\n\n\nLoad the ZIP Code Shapefile\n\n::: {#c33f88e5 .cell execution_count=21}\n``` {.python .cell-code}\n#zip_shapefile = gpd.read_file('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/ZIP_Code_Shapefiles/tl_2015_us_zcta510.shp')\n\n#zip_shapefile = zip_shapefile.to_crs(\"EPSG:4326\")\n```\n:::\n\n\nSpatial join to match the zip codes\n\n::: {#eaf64c21 .cell execution_count=22}\n``` {.python .cell-code}\n#weather_with_zip = gpd.sjoin(weather_gdf, zip_shapefile, how='left', predicate='intersects')\n\n#print(weather_with_zip.head())\n```\n:::\n\n\nDrop irrelevant dataframes: \n\n::: {#e80fcc89 .cell execution_count=23}\n``` {.python .cell-code}\n#weather_with_zip.drop([\"CLASSFP10\", \"MTFCC10\", \"FUNCSTAT10\", \"ALAND10\", \"AWATER10\", \"GEOID10\"], axis=1, inplace=True)\n\n#print(weather_with_zip)\n```\n:::\n\n\nFinal CSV file for weather \n\n::: {#24797aff .cell execution_count=24}\n``` {.python .cell-code}\n#weather_with_zip.to_csv(\"weather_with_zip.csv\", index=False)\n```\n:::\n\n\nLoading CSV file \n\n::: {#781d3e97 .cell execution_count=25}\n``` {.python .cell-code}\n#weatherfinal = pd.read_csv('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/Final project/Weather_Data/weather_with_zip.csv')\n```\n:::\n\n\nDropping unnecessary columns\n\n::: {#83772a9f .cell execution_count=26}\n``` {.python .cell-code}\n#weatherfinal.head(10)\n#weatherfinal = weatherfinal.drop(columns=['TMAX', 'TMIN', 'ELEVATION', 'index_right'])\n```\n:::\n\n\nSaving to a CSV file \n\n::: {#7f1f67a5 .cell execution_count=27}\n``` {.python .cell-code}\n#weatherfinal.to_csv('/Users/jennyzhong/Documents/GitHub/final-Project-DAP-II/Final project/Weather_Data/weather_raw.csv', index=False)\n```\n:::\n\n\n## Data experiment\n\n\n## Data Visualization\n#merging the data\n#For the following data, we decided to filter the information down to \n#2010-2015 because 30 GB is way too much, the following code is how we \n#shrunk the data \n\n::: {#649b443c .cell execution_count=28}\n``` {.python .cell-code}\n##installing the necessary packages\n#import pandas as pd\n#import matplotlib.pyplot as plt\n#import altair as alt\n#import numpy as np\n#import geopandas as gpd\n#import shapely\n#import shiny\n\n\n#from shapely.geometry import Point\n#import matplotlib.pyplot as plt\n\n#from shapely.geometry import Point\n#import pgeocode\n```\n:::\n\n\n## Data experiment\n\n\n## Data Visualization\n\nmerging the data\nFor the following data, we decided to filter the information down to \n2010-2015 because 30 GB is way too much, the following code is how we \nshrunk the data \n\n::: {#306c1601 .cell execution_count=29}\n``` {.python .cell-code}\n#final_crime_data = pd.read_csv(r'C:\\Users\\madig\\Documents\\Github\\Year 2024-2025\\final-Project-DAP-II\\final_crime_data.csv')\n#final_weather_data = pd.read_csv(r'C:\\Users\\madig\\Documents\\Github\\Year 2024-2025\\final-Project-DAP-II\\final_weather_data.csv')\n```\n:::\n\n\n::: {#33acffb0 .cell execution_count=30}\n``` {.python .cell-code}\n#final_crime_data = pd.read_csv(r'C:\\Users\\madig\\Documents\\Github\\Year 2024-2025\\final-Project-DAP-II\\final_crime_data.csv')\n#final_crime_data['DATE'] = pd.to_datetime(final_crime_data['YearMonthDay'])\n#filtered_crime_data = final_crime_data[\n#    (final_crime_data['DATE'].dt.year >= 2010) & (final_crime_data['DATE'].dt.year <= 2015)\n#]\n#filtered_crime_data.to_csv('filtered_crime_data.csv', index=False)\n```\n:::\n\n\nSo this is the filtered data from 2010-2015\n\nCrime data grouping and removing unneccesary columns \n\nAnd making a function to group crime types\n\n::: {#aea8588f .cell execution_count=31}\n``` {.python .cell-code}\n#filtered_weather_data = pd.read_csv(r'C:\\Users\\madig\\Documents\\Github\\Year 2024-2025\\final-Project-DAP-II\\Final project\\weatherfinal_cleaned.csv')\n#filtered_crime_data = pd.read_csv(r'C:\\Users\\madig\\Documents\\Github\\Year 2024-2025\\final-Project-DAP-II\\Final project\\filtered_crime_data.csv')\n```\n:::\n\n\n::: {#750d8745 .cell execution_count=32}\n``` {.python .cell-code}\n#unique_primary_types = filtered_crime_data['Primary Type'].unique()\n\n#def categorize_crime(crime_type):\n    #if crime_type in ['BATTERY', 'ASSAULT', 'ROBBERY', 'SEX OFFENSE', \n                      #'CRIM SEXUAL ASSAULT', 'CRIMINAL SEXUAL ASSAULT', \n                      #'HOMICIDE', 'KIDNAPPING', 'STALKING', 'INTIMIDATION']:\n        #return 'Violent Crimes'\n    #elif crime_type in ['MOTOR VEHICLE THEFT', 'THEFT', 'BURGLARY', \n                        #'CRIMINAL DAMAGE', 'ARSON', 'CRIMINAL TRESPASS']:\n        #return 'Property Crimes'\n    #elif crime_type in ['WEAPONS VIOLATION', 'PUBLIC PEACE VIOLATION', \n                        #'LIQUOR LAW VIOLATION', 'OBSCENITY', \n                        #'PUBLIC INDECENCY', 'CONCEALED CARRY LICENSE VIOLATION']:\n       # return 'Public Order and Regulatory Violations'\n    #else:\n        #return 'Other Offenses'\n\n#filtered_crime_data['Crime Type Group'] = filtered_crime_data['Primary Type'].apply(categorize_crime)\n\n#columns_to_drop = [\n    #'YearMonthDay', 'Block', 'IUCR', 'Primary Type', 'Description',\n    #'Location Description', 'Arrest', 'Domestic', 'Beat', 'Ward',\n    #'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Location',\n    #'ID', 'District', 'Community Area', 'index_right', 'INTPTLAT10', \n    #'INTPTLON10'\n#]\n\n#filtered_crime_data = filtered_crime_data.drop(columns=columns_to_drop)\n\n#filtered_crime_data = filtered_crime_data.dropna(subset=['ZCTA5CE10'])\n#filtered_crime_data['ZCTA5CE10'] = filtered_crime_data['ZCTA5CE10'].astype(int)\n\n#filtered_crime_data.rename(columns={'ZCTA5CE10': 'ZIP_CODE'}, inplace=True)\n\n#print(filtered_crime_data.head())\n```\n:::\n\n\nMerging two datasets (crime and weather) by Zipcode and Date column.\n\n::: {#b5a00aab .cell execution_count=33}\n``` {.python .cell-code}\n#filtered_crime_data['DATE'] = pd.to_datetime(filtered_crime_data['DATE'])\n#filtered_weather_data['DATE'] = pd.to_datetime(filtered_weather_data['DATE'])\n\n#crime_summary = filtered_crime_data.groupby(['DATE', 'ZIP_CODE', 'Crime Type Group']).size().reset_index(name='Crime Count')\n\n\n#merged_data = pd.merge(crime_summary,\n    #filtered_weather_data[['DATE', 'ZIP_CODE', 'TAVG', 'geometry']], \n    #on=['DATE', 'ZIP_CODE'],\n    #how='inner')\n\n#print(merged_data.head())\n\n#merged_data.to_csv(r'C:\\Users\\madig\\Documents\\Github\\Year 2024-2025\\final-Project-DAP-II\\Final project\\merged_data.csv', index=False)\n```\n:::\n\n\nNow that the data is filtered lets make the two charts\n\nBarChart:\n\n::: {#f0b0841c .cell execution_count=34}\n``` {.python .cell-code}\n#import pandas as pd\n#import matplotlib.pyplot as plt\n\n#filtered_weather_data['TAVG_bin'] = pd.cut(\n    #filtered_weather_data['TAVG'], \n    #bins=range(int(filtered_weather_data['TAVG'].min()) // 15 * 15, \n               #int(filtered_weather_data['TAVG'].max()) // 15 * 15 + 15, 15), \n   # right=False\n#)\n\n#merged_data_with_bins = pd.merge(merged_data, filtered_weather_data[['DATE', 'TAVG_bin']], on='DATE',how='left')\n\n#crime_distribution = merged_data_with_bins.groupby(['TAVG_bin', 'Crime Type Group'])['Crime Count'].sum().unstack(fill_value=0)\n\n#crime_distribution.plot(kind='bar', stacked=True, figsize=(12, 7), width=0.9)\n#plt.title('Crime Distribution by Temperature Range (Binned by 15)')\n#plt.xlabel('Temperature Range (°F)')\n#plt.ylabel('Total Crime Count')\n#plt.xticks(rotation=45, ha='right')  \n#plt.legend(title='Crime Type Group')\n#plt.grid(axis='y', linestyle='--', alpha=0.7)\n#plt.tight_layout()\n#plt.show()\n```\n:::\n\n\nGeopandas Timeseries\n\n::: {#4690c7e0 .cell execution_count=35}\n``` {.python .cell-code}\n#import geopandas as gpd\n#import matplotlib.pyplot as plt\n#import pandas as pd\n\n#merged_data = gpd.GeoDataFrame(merged_data)\n\n#merged_data[\"DATE\"] = pd.to_datetime(merged_data[\"DATE\"])\n\n#time_series_data = merged_data.groupby([\"DATE\", \"Crime Type Group\"])[\"Crime Count\"].sum().unstack()\n\n#plt.figure(figsize=(12, 6))\n#time_series_data.plot(ax=plt.gca(), marker='o')\n#plt.title(\"Time Series of Crime Counts by Type\", fontsize=14)\n#plt.xlabel(\"Date\", fontsize=12)\n#plt.ylabel(\"Crime Count\", fontsize=12)\n#plt.legend(title=\"Crime Type Group\")\n#plt.grid(True)\n#plt.tight_layout()\n#plt.show()\n```\n:::\n\n\nCloropleth Map\n\n::: {#3ea136bb .cell execution_count=36}\n``` {.python .cell-code}\n#from shapely.wkt import loads\n\n#merged_data['geometry'] = merged_data['geometry'].apply(loads)\n```\n:::\n\n\n::: {#79b622ed .cell execution_count=37}\n``` {.python .cell-code}\n#import matplotlib.pyplot as plt\n#import contextily as ctx\n\n#merged_data = merged_data.set_geometry(\"geometry\")\n\n#if merged_data.crs is None:\n    #merged_data = merged_data.set_crs(epsg=4326)\n#merged_data = merged_data.to_crs(epsg=3857)\n\n#fig, ax = plt.subplots(figsize=(12, 8))\n\n#merged_data.plot(\n    #column=\"TAVG\",\n    #cmap=\"coolwarm\",\n    #legend=True,\n    #alpha=0.6,\n    #edgecolor=\"black\",\n    #ax=ax,\n#)\n\n\n#merged_data.plot(\n    #ax=ax,\n    #color=\"red\",\n    #markersize=merged_data[\"Crime Count\"] * 2,\n   # alpha=0.7,\n    #label=\"Crime Locations\",\n#)\n\n\n#ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#plt.title(\"Choropleth Map of Temp with Crime Locations\", fontsize=14)\n#plt.legend()\n#plt.tight_layout()\n#plt.show()\n```\n:::\n\n\nWe explain the following graphs here: \n\nThis bar chart demonstrates crime frequency across temperature ranges, revealing a significant increase in crimes at higher temperatures. This aligns with routine activity theory, as warmer temperatures encourage outdoor activities, potentially increasing encounters between offenders and victims. Conversely, fewer crimes generally occur in colder temperature ranges,, likely due to reduced outdoor activities and interactions.\n![Graph 1 Description](Graphs/graph1.jpeg)\n\nThis chart illustrates the trend of crime occurrences over time, highlighting a noticeable increase during the summer months, suggesting a seasonal pattern in criminal activity.\n![Graph 2 Description](Graphs/Graph2.jpeg)\n\nThis choropleth map displays crime locations in Chicago, with points indicating crime occurrences and their color representing the temperature. Red points signify higher temperatures, while blue points indicate lower temperatures. The map shows that red points are more prominent, suggesting that crimes are more frequent during warmer weather. This highlights a possible connection between temperature and crime rates.\n![Graph 3 Description](Graphs/Graph3.jpeg)\n\n\n## Shiny App\n\nplease refer to the basic-app/app.py for the shiny app.\nhere are the photos from the shiny app:\n\n![ the average crime rate per zipcode on ](low_temp.png)\n\n\n![the intensity of crime per zipcode when weather is ](High_temp.png)\n\n\n![the intensity of crime per zipcode in chicago when temperature is 89](extra_high.png)\n\n\n\n## Policy Implications \n\nOur findings demonstrate a positive correlation between temperature and violent crime rates, aligning with prior research on the relationship between environmental factors and criminal activity. This connection underscores the importance of understanding how temperature fluctuations influence public behavior and safety.\nFrom a public safety perspective, this insight allows police departments to better anticipate “busy seasons” for violent crimes, particularly during warmer months. With this knowledge, law enforcement agencies can optimize resource allocation, ensuring adequate staffing and preparation to handle increased workloads effectively.\nAdditionally, the implications extend to the broader context of climate change. As global temperatures continue to rise, it becomes increasingly critical to assess and address the potential impacts on public safety. Policymakers can use this information to formulate strategies that mitigate the risks associated with climate-induced changes in crime patterns, contributing to more resilient urban environments.\n\n## Directions for Future Work\n\nTo broaden the scope and applicability of our findings, future research should expand the analysis to other American cities. This would help determine whether the patterns observed in Chicago are consistent across different urban contexts, offering a more comprehensive understanding of how temperature impacts crime in diverse environments.\nA global analysis could further enhance the study by assessing whether the relationship between temperature and crime is universal or shaped by cultural and geographic factors. Examining this phenomenon in international settings would provide valuable insights into the broader applicability of our findings and reveal potential variations influenced by societal norms and regional climates.\nAdditionally, incorporating other variables such as humidity, unemployment rates, and urban density could create a more robust and nuanced model. These factors may interact with temperature in complex ways, offering a deeper understanding of the environmental and socio-economic drivers of crime. By expanding this research, we can gain a more thorough understanding of the intersection between climate, public safety, and social behavior, ultimately contributing to more informed policy and crime prevention strategies.\n\n",
    "supporting": [
      "Final_project_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}